# rss/fetch_rss.py

import os
import feedparser
import json
from datetime import datetime, timezone
from typing import Dict, List
import hashlib
import re
import sys
import socket
from concurrent.futures import ThreadPoolExecutor

# --- æ–°å¢ä¾èµ–ï¼Œè¯·å…ˆå®‰è£…: pip install requests beautifulsoup4 lxml bleach ---
import requests
from bs4 import BeautifulSoup
import bleach


# --- 1. å…¨å±€é…ç½® ---

# -- ç‹¬ç«‹çš„å¤§ç±»é…ç½® --
CATEGORIES = {
    "æŠ€æœ¯": {"icon": "ğŸ’»", "color": "#4A90E2"},
    "å·¥ä½œ": {"icon": "ğŸ¢", "color": "#50C878"},
    "èµ„è®¯": {"icon": "ğŸŒ", "color": "#FF6B6B"}
}

# -- RSS æºï¼ˆå­ç±»ï¼‰é…ç½® --
# æ–°å¢å‚æ•°è¯´æ˜:
# fetch_full_content (bool): æ˜¯å¦éœ€è¦äºŒæ¬¡æŠ“å–ç½‘é¡µä»¥è·å–å…¨æ–‡ã€‚
#   - True:  é€‚ç”¨äºRSSåªæä¾›æ‘˜è¦ï¼Œéœ€è¦è®¿é—®åŸæ–‡é“¾æ¥è·å–å…¨æ–‡çš„ç½‘ç«™ã€‚
#   - False: é€‚ç”¨äºRSSæœ¬èº«å°±æä¾›å…¨æ–‡ï¼ˆå¦‚V2EXï¼‰ï¼Œæˆ–æ— æ³•/ä¸æƒ³æŠ“å–å…¨æ–‡çš„æƒ…å†µã€‚
# content_selector (str):  å½“ fetch_full_content ä¸º True æ—¶ï¼Œç”¨äºæå–æ­£æ–‡çš„CSSé€‰æ‹©å™¨ã€‚
#   - è¿™æ˜¯æœ€å…³é”®çš„é…ç½®ï¼Œéœ€è¦é’ˆå¯¹æ¯ä¸ªç½‘ç«™çš„HTMLç»“æ„è¿›è¡Œåˆ†æã€‚
# sanitize_summary (bool): æ˜¯å¦å¯¹RSSæºä¸­çš„summary/descriptionå­—æ®µè¿›è¡ŒHTMLå‡€åŒ–ï¼Œè€Œä¸æ˜¯ç²—æš´ç§»é™¤æ ‡ç­¾ã€‚
#   - True:  ä¿ç•™æ‘˜è¦ä¸­çš„æ ¼å¼ï¼ˆåŠ ç²—ã€é“¾æ¥ã€å›¾ç‰‡ç­‰ï¼‰ï¼Œæå‡é˜…è¯»ä½“éªŒã€‚
RSS_FEEDS = {
    "ç¾å›¢æŠ€æœ¯å›¢é˜Ÿ": {
        "url": "https://tech.meituan.com/feed/",
        "category": "æŠ€æœ¯",
        "icon": "ğŸš€",
        "color": "#FFD93D",
        "description": "ç¾å›¢æŠ€æœ¯å›¢é˜Ÿåšå®¢",
        "fetch_full_content": True,
        "content_selector": "div.post-content", # ç¾å›¢æ–‡ç« æ­£æ–‡åœ¨<div class="post-content">ä¸­
        "sanitize_summary": False
    },
    "æ½®æµå‘¨åˆŠ": {
        "url": "https://weekly.tw93.fun/rss.xml",
        "category": "èµ„è®¯",
        "icon": "ğŸ“°",
        "color": "#FCA5A5",
        "description": "å‰ç«¯æ½®æµæŠ€æœ¯å‘¨åˆŠ",
        "fetch_full_content": False, # RSSå†…å®¹å·²æ˜¯å…¨æ–‡
        "sanitize_summary": True # éœ€è¦å‡€åŒ–HTMLä»¥ä¿ç•™æ ¼å¼
    },
    "V2EXæŠ€æœ¯ä¸“åŒº": {
        "url": "https://www.v2ex.com/feed/tab/tech.xml",
        "category": "æŠ€æœ¯",
        "icon": "ğŸ”§",
        "color": "#A5B4FC",
        "description": "V2EXæŠ€æœ¯è®¨è®ºåŒº",
        "fetch_full_content": False, # V2EXçš„descriptionå°±æ˜¯å¸–å­å†…å®¹
        "sanitize_summary": True # éœ€è¦å‡€åŒ–HTMLæ¥å±•ç¤ºå¸–å­å†…å®¹
    },
    "V2EXé…·å·¥ä½œ": {
        "url": "https://www.v2ex.com/feed/tab/jobs.xml",
        "category": "å·¥ä½œ",
        "icon": "ğŸ’¼",
        "color": "#A7F3D0",
        "description": "V2EXæ‹›è˜ä¿¡æ¯",
        "fetch_full_content": False,
        "sanitize_summary": True
    }
}

# -- å…¶ä»–è®¾ç½® --
MAX_ENTRIES_LIMIT = 200
ENTRIES_PER_PAGE = 20
FETCH_CONCURRENCY = 5  # å¹¶å‘æŠ“å–çº¿ç¨‹æ•°
REQUEST_TIMEOUT = 15   # ç½‘ç»œè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# æ¨¡æ‹Ÿæ™®é€š Windows 10 ä¸Š Chrome æµè§ˆå™¨çš„è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    # --- æ–°å¢/ä¿®æ”¹çš„å¤´ ---
    'Referer': 'https://tech.meituan.com/', # ä¼ªé€ ä¸€ä¸ªæ¥æºé¡µ
    'DNT': '1', # Do Not Track
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'same-origin',
    'Sec-Fetch-User': '?1',
}
BALANCE_STRATEGIES = {"equal": "å¹³å‡åˆ†é…", "weighted": "æŒ‰æƒé‡åˆ†é…", "dynamic": "åŠ¨æ€åˆ†é…ï¼ˆåŸºäºæ´»è·ƒåº¦ï¼‰"}
RSS_WEIGHTS = {"V2EXæŠ€æœ¯ä¸“åŒº": 3, "ç¾å›¢æŠ€æœ¯å›¢é˜Ÿ": 3, "V2EXé…·å·¥ä½œ": 2, "æ½®æµå‘¨åˆŠ": 2}


# --- 2. è¾…åŠ©å‡½æ•° ---

def get_output_path():
    """è®¡ç®—å¹¶è¿”å›JSONæ–‡ä»¶çš„ç»å¯¹è·¯å¾„"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    data_dir = os.path.join(project_root, 'data')
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    return os.path.join(data_dir, 'friends_feed.json')

def parse_date(date_string):
    """è§£æå¤šç§æ—¥æœŸæ ¼å¼"""
    formats = [
        "%a, %d %b %Y %H:%M:%S %z", "%a, %d %b %Y %H:%M:%S %Z",
        "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S.%f%z",
    ]
    for fmt in formats:
        try:
            if ":" == date_string[-3:-2]: date_string = date_string[:-3] + date_string[-2:]
            return datetime.strptime(date_string, fmt)
        except ValueError:
            pass
    return datetime(1970, 1, 1, tzinfo=timezone.utc)

def generate_entry_id(link: str) -> str:
    """ä¸ºæ–‡ç« ç”Ÿæˆå”¯ä¸€ID"""
    return hashlib.md5(link.encode()).hexdigest()[:12]

def sanitize_html(html_content: str) -> str:
    """ä½¿ç”¨bleachå‡€åŒ–HTMLï¼Œä¿ç•™å®‰å…¨æ ‡ç­¾å’Œæ ¼å¼"""
    allowed_tags = [
        'p', 'br', 'a', 'img', 'video', 'audio',
        'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
        'strong', 'em', 'u', 's', 'b', 'i',
        'ul', 'ol', 'li', 'blockquote',
        'pre', 'code', 'figure', 'figcaption'
    ]
    allowed_attrs = {
        '*': ['class', 'style'],
        'a': ['href', 'title', 'target'],
        'img': ['src', 'alt', 'title', 'width', 'height', 'loading'],
        'video': ['src', 'controls', 'width', 'height'],
        'audio': ['src', 'controls'],
    }
    return bleach.clean(html_content, tags=allowed_tags, attributes=allowed_attrs, strip=True)

def fetch_full_content(url: str, selector: str) -> str:
    """æŠ“å–å¹¶è§£æç½‘é¡µï¼Œæå–æŒ‡å®šéƒ¨åˆ†HTML"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        response.encoding = response.apparent_encoding # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
        soup = BeautifulSoup(response.text, 'lxml')
        content_element = soup.select_one(selector)
        if content_element:
            # å¯ä»¥åœ¨è¿™é‡Œåšä¸€äº›æ¸…ç†ï¼Œä¾‹å¦‚ç§»é™¤ä¸æƒ³å…³çš„å­å…ƒç´ 
            return str(content_element)
    except requests.exceptions.RequestException as e:
        print(f"    -> æŠ“å–å…¨æ–‡å¤±è´¥: {url}, é”™è¯¯: {e}")
    except Exception as e:
        print(f"    -> è§£æå…¨æ–‡å¤±è´¥: {url}, é”™è¯¯: {e}")
    return ""


# --- 3. æ ¸å¿ƒæŠ“å–ä¸å¤„ç†é€»è¾‘ ---

def fetch_and_process_feed(args) -> List[dict]:
    """æŠ“å–å¹¶å¤„ç†å•ä¸ªRSSæºï¼ˆè®¾è®¡ä¸ºå¯å¹¶å‘è°ƒç”¨ï¼‰"""
    blog_name, feed_config, max_entries = args
    entries = []
    feed_url = feed_config["url"]
    
    print(f"  å¤„ç†ä¸­: {blog_name} (é…é¢: {max_entries})")
    
    try:
        # ä½¿ç”¨å…¨å±€socketè¶…æ—¶æ¥æ§åˆ¶feedparserçš„è¯·æ±‚
        socket.setdefaulttimeout(REQUEST_TIMEOUT)
        feed = feedparser.parse(feed_url, agent=HEADERS.get('User-Agent'))
        
        if feed.bozo:
            bozo_exception = feed.get('bozo_exception', 'æœªçŸ¥é”™è¯¯')
            print(f"    -> è­¦å‘Š: '{blog_name}' RSS æºæ ¼å¼ä¸æ­£ç¡®ã€‚é”™è¯¯: {bozo_exception}")

        # æŒ‰å‘å¸ƒæ—¥æœŸæ’åº
        feed_entries = sorted(feed.entries, key=lambda x: parse_date(x.get("published", x.get("updated", ""))), reverse=True)
        
        for entry in feed_entries[:max_entries]:
            published_str = entry.get("published", entry.get("updated"))
            if not published_str: continue
            
            dt_object = parse_date(published_str)
            summary_html = entry.get("summary", entry.get("description", ""))

            # --- å…¨æ–‡è·å–ä¸å†…å®¹å¤„ç† ---
            summary = ""
            content = ""

            # 1. ä¼˜å…ˆè·å–å…¨æ–‡
            if feed_config.get("fetch_full_content") and feed_config.get("content_selector"):
                content_html = fetch_full_content(entry.link, feed_config["content_selector"])
                if content_html:
                    content = sanitize_html(content_html)
            
            # 2. å¦‚æœæ²¡æœ‰å…¨æ–‡ï¼Œæˆ–é…ç½®äº†å‡€åŒ–æ‘˜è¦ï¼Œåˆ™å¤„ç†æ‘˜è¦
            if feed_config.get("sanitize_summary", False):
                summary = sanitize_html(summary_html)
            else:
                summary = re.sub(r'<[^>]+>', '', summary_html).strip()[:200]
            
            # å¦‚æœcontentä¸ºç©ºï¼Œä½†å‡€åŒ–åçš„summaryä¸ä¸ºç©ºï¼Œåˆ™å°†summaryä½œä¸ºcontent
            if not content and feed_config.get("sanitize_summary", False):
                content = summary

            # æå–ä½œè€…å’Œæ ‡ç­¾
            author = entry.get("author", "æœªçŸ¥")
            tags = [tag.get('term') for tag in entry.get("tags", [])]
            
            entries.append({
                "id": generate_entry_id(entry.link),
                "blog_name": blog_name, "title": entry.title, "link": entry.link,
                "published": dt_object.isoformat(), "timestamp": int(dt_object.timestamp()),
                "summary": summary,
                "content": content, # << æ–°å¢å®Œæ•´å†…å®¹å­—æ®µ
                "author": author,   # << æ–°å¢ä½œè€…å­—æ®µ
                "tags": tags,       # << æ–°å¢æ ‡ç­¾å­—æ®µ
                "category": feed_config.get("category", "å…¶ä»–"),
                "source_icon": feed_config.get("icon", "ğŸ“„"), "source_color": feed_config.get("color", "#666666")
            })
        print(f"    -> æˆåŠŸå¤„ç† {len(entries)} ç¯‡æ–‡ç« ")
    except Exception as e:
        print(f"    -> é”™è¯¯: æŠ“å–æˆ–å¤„ç† '{blog_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
    return entries


# --- 4. åˆ†é…ç­–ç•¥å‡½æ•° (ä¸åŸç‰ˆç›¸åŒï¼Œä¸ºä¿æŒå®Œæ•´æ€§è€Œä¿ç•™) ---

def calculate_equal_allocation(feed_count: int, total_limit: int) -> Dict[str, int]:
    if feed_count == 0: return {}
    base = total_limit // feed_count
    rem = total_limit % feed_count
    return {name: base + (1 if i < rem else 0) for i, name in enumerate(RSS_FEEDS.keys())}

def calculate_weighted_allocation(total_limit: int) -> Dict[str, int]:
    total_weight = sum(RSS_WEIGHTS.values())
    if total_weight == 0: return calculate_equal_allocation(len(RSS_FEEDS), total_limit)
    allocation = {name: int(total_limit * (RSS_WEIGHTS.get(name, 1) / total_weight)) for name in RSS_FEEDS.keys()}
    # ä¿®æ­£å› å–æ•´å¯¼è‡´çš„å’Œä¸ç­‰äºtotal_limitçš„é—®é¢˜
    current_total = sum(allocation.values())
    diff = total_limit - current_total
    for i in range(diff): allocation[list(RSS_FEEDS.keys())[i % len(RSS_FEEDS)]] += 1
    return allocation

def calculate_dynamic_allocation(existing_entries: List[dict], total_limit: int) -> Dict[str, int]:
    counts = {name: 0 for name in RSS_FEEDS.keys()}
    for entry in existing_entries:
        if entry.get("blog_name") in counts: counts[entry["blog_name"]] += 1
    total = sum(counts.values())
    if total == 0: return calculate_equal_allocation(len(RSS_FEEDS), total_limit)
    
    # åŠ¨æ€åˆ†é…ï¼ŒåŸºäºå†å²æ–‡ç« æ¯”ä¾‹ï¼Œä½†ä¿è¯æ¯ä¸ªæºè‡³å°‘æœ‰1ä¸ªé…é¢
    ratios = {name: count / total for name, count in counts.items()}
    allocation = {name: max(1, int(ratio * total_limit)) for name, ratio in ratios.items()}
    
    # ä¿®æ­£æ€»æ•°
    current_total = sum(allocation.values())
    while current_total < total_limit:
        # å°†å‰©ä½™é…é¢åŠ åˆ°æ¯”ä¾‹æœ€é«˜çš„æºä¸Š
        max_ratio_feed = max(ratios, key=ratios.get)
        allocation[max_ratio_feed] += 1
        current_total += 1
    return allocation

def get_allocation_strategy(existing_entries: List[dict], strategy: str) -> Dict[str, int]:
    if strategy == "equal": return calculate_equal_allocation(len(RSS_FEEDS), MAX_ENTRIES_LIMIT)
    if strategy == "weighted": return calculate_weighted_allocation(MAX_ENTRIES_LIMIT)
    if strategy == "dynamic": return calculate_dynamic_allocation(existing_entries, MAX_ENTRIES_LIMIT)
    return calculate_dynamic_allocation(existing_entries, MAX_ENTRIES_LIMIT)


# --- 5. ä¸»å‡½æ•° ---

def main(strategy: str):
    output_path = get_output_path()
    
    # æ­¥éª¤ 1: è¯»å–å†å²æ•°æ®
    print("--- 1. æ­£åœ¨è¯»å–å†å²æ•°æ®... ---")
    existing_entries = []
    if os.path.exists(output_path):
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                old_data = json.load(f)
                existing_entries = old_data.get('articles', [])
            print(f"æˆåŠŸåŠ è½½ {len(existing_entries)} æ¡å†å²æ–‡ç« ã€‚")
        except (json.JSONDecodeError, IOError) as e:
            print(f"è­¦å‘Š: è¯»å–æˆ–è§£ææ—§æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")

    # æ­¥éª¤ 2: è®¡ç®—åˆ†é…ç­–ç•¥
    print(f"\n--- 2. è®¡ç®—è´Ÿè½½å‡è¡¡åˆ†é… (ç­–ç•¥: {BALANCE_STRATEGIES.get(strategy, strategy)}) ---")
    allocation = get_allocation_strategy(existing_entries, strategy)
    print("åˆ†é…ç»“æœ:")
    for name, count in allocation.items(): print(f"  {name}: {count} ç¯‡")
    
    # æ­¥éª¤ 3: å¹¶å‘æŠ“å–æ–°æ•°æ®
    print(f"\n--- 3. æ­£åœ¨å¹¶å‘æŠ“å–RSS feeds (å¹¶å‘æ•°: {FETCH_CONCURRENCY})... ---")
    all_new_entries = []
    tasks = [(name, config, allocation.get(name, 0)) for name, config in RSS_FEEDS.items() if allocation.get(name, 0) > 0]
    
    with ThreadPoolExecutor(max_workers=FETCH_CONCURRENCY) as executor:
        results = executor.map(fetch_and_process_feed, tasks)
        for result in results:
            all_new_entries.extend(result)
            
    print(f"æŠ“å–å®Œæˆï¼Œå…±è·å¾— {len(all_new_entries)} ç¯‡æ–‡ç« ã€‚")
    
    # æ­¥éª¤ 4: åˆå¹¶ä¸å»é‡
    print("\n--- 4. å»é‡ä¸åˆå¹¶... ---")
    combined_entries = {e['link']: e for e in existing_entries}
    new_count = 0
    for entry in all_new_entries:
        if entry['link'] not in combined_entries:
            new_count += 1
        combined_entries[entry['link']] = entry

    # æ’åºå¹¶æˆªæ–­
    final_entries = sorted(combined_entries.values(), key=lambda x: x.get('timestamp', 0), reverse=True)[:MAX_ENTRIES_LIMIT]
    print(f"æ–°å¢ {new_count} ç¯‡æ–‡ç« ï¼Œå»é‡å’Œæˆªæ–­åï¼Œæœ€ç»ˆå…± {len(final_entries)} ç¯‡ã€‚")
    
    # æ­¥éª¤ 5: ç”Ÿæˆå…ƒæ•°æ® (ä¸åŸç‰ˆç±»ä¼¼ï¼Œä½†æ›´å¥å£®)
    categories_meta = {name: {"icon": conf.get("icon", "ğŸ“"), "color": conf.get("color", "#666"), "count": 0, "sources": {}}
                       for name, conf in CATEGORIES.items()}
    source_counts = {name: 0 for name in RSS_FEEDS.keys()}
    for entry in final_entries:
        if entry.get('blog_name') in source_counts:
            source_counts[entry['blog_name']] += 1
    
    for src_name, src_config in RSS_FEEDS.items():
        cat_name = src_config.get('category')
        if cat_name in categories_meta:
            count = source_counts.get(src_name, 0)
            categories_meta[cat_name]['sources'][src_name] = {
                "icon": src_config.get("icon", "ğŸ“„"), "color": src_config.get("color", "#888"),
                "description": src_config.get("description", ""), "count": count
            }
            categories_meta[cat_name]['count'] += count

    # æ„å»ºæœ€ç»ˆè¾“å‡ºæ•°æ®
    output_data = {
        "meta": {
            "total_articles": len(final_entries), "last_updated": datetime.now(timezone.utc).isoformat(),
            "entries_per_page": ENTRIES_PER_PAGE, "categories": categories_meta
        },
        "articles": final_entries
    }
    
    # æ­¥éª¤ 6: å†™å…¥æ–‡ä»¶
    print("\n--- 5. ä¿å­˜åˆ°æ–‡ä»¶... ---")
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"æˆåŠŸï¼æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        
        print("\n--- æœ€ç»ˆç»Ÿè®¡ ---")
        for cat, info in categories_meta.items():
            if info['count'] > 0:
                print(f"  {info['icon']} {cat}: {info['count']} ç¯‡")
                for src, s_info in info['sources'].items():
                    if s_info['count'] > 0:
                        print(f"    - {s_info['icon']} {src}: {s_info['count']} ç¯‡")
    except IOError as e:
        print(f"é”™è¯¯ï¼æ— æ³•å†™å…¥æ–‡ä»¶: {e}")

# --- 6. è„šæœ¬æ‰§è¡Œå…¥å£ ---

if __name__ == "__main__":
    strategy_arg = "dynamic"
    if len(sys.argv) > 1 and sys.argv[1] in BALANCE_STRATEGIES:
        strategy_arg = sys.argv[1]
    
    print(f"ä½¿ç”¨è´Ÿè½½å‡è¡¡ç­–ç•¥: {BALANCE_STRATEGIES.get(strategy_arg, 'æœªçŸ¥')}")
    print("=" * 60)
    
    main(strategy_arg)